# Instalação de dependências
!pip install -q tensorflow tensorflow-recommenders tensorflow-addons annoy matplotlib

# Importação de bibliotecas
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
import matplotlib.pyplot as plt
from PIL import Image
import urllib.request
import annoy

# Configurações
IMG_SIZE = (224, 224)
EMBEDDING_DIM = 2048  # Dimensão para ResNet50
ANN_TREES = 10

# 1. Pré-processamento de imagens
def load_and_preprocess(image_path):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, IMG_SIZE)
    return tf.keras.applications.resnet50.preprocess_input(image)

# 2. Carregar dataset de exemplo (sapatos)
dataset_url = 'http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ut-zap50k-images-square.zip'
urllib.request.urlretrieve(dataset_url, 'shoes.zip')
!unzip -q shoes.zip -d dataset

# 3. Criar lista de caminhos de imagens
image_paths = []
for root, _, files in os.walk('dataset'):
    for file in files:
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            image_paths.append(os.path.join(root, file))

# 4. Modelo de extração de características
base_model = tf.keras.applications.ResNet50(
    weights='imagenet',
    include_top=False,
    pooling='avg'
)
model = tf.keras.Sequential([
    base_model,
    layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))
])

# 5. Gerar embeddings para todas as imagens
def generate_embeddings(image_paths, batch_size=32):
    dataset = tf.data.Dataset.from_tensor_slices(image_paths)
    dataset = dataset.map(load_and_preprocess)
    dataset = dataset.batch(batch_size)
    return model.predict(dataset)

embeddings = generate_embeddings(image_paths)

# 6. Construir índice de similaridade
index = annoy.AnnoyIndex(EMBEDDING_DIM, 'angular')
for i, emb in enumerate(embeddings):
    index.add_item(i, emb)
index.build(ANN_TREES)

# 7. Função de recomendação
def recommend_similar(image_url, num_results=5):
    # Baixar imagem
    local_path = 'query.jpg'
    urllib.request.urlretrieve(image_url, local_path)
    
    # Processar imagem
    img = load_and_preprocess(local_path)
    query_embedding = model.predict(tf.expand_dims(img, axis=0))[0]
    
    # Buscar similares
    similar_ids = index.get_nns_by_vector(query_embedding, num_results)
    
    # Exibir resultados
    plt.figure(figsize=(15,5))
    plt.subplot(1, num_results+1, 1)
    plt.imshow(Image.open(local_path))
    plt.title('Query Image')
    plt.axis('off')
    
    for i, idx in enumerate(similar_ids):
        plt.subplot(1, num_results+1, i+2)
        plt.imshow(Image.open(image_paths[idx]))
        plt.title(f'Similar {i+1}')
        plt.axis('off')
    plt.show()

# Exemplo de uso
recommend_similar('https://example.com/query-shoe-image.jpg')
